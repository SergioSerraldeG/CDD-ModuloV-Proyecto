import os
import streamlit as st
from dotenv import load_dotenv
import google.generativeai as genai
from prompts import stronger_prompt

# Cargar variables de entorno desde el archivo .env
load_dotenv(override=True)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Configurar el cliente de Gemini
genai.configure(api_key=GOOGLE_API_KEY)
model_gemini = genai.GenerativeModel("gemini-2.5-pro")

st.title("游늵 FinguIA")
st.caption("游눯 Inversiones simplificadas.")

# Inicializar el historial de mensajes en el estado de la sesi칩n si no existe
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "쮼n qu칠 te puedo ayudar?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input(placeholder="Escribe tu mensaje aqu칤..."):
    # A침adir el mensaje del usuario al historial y mostrarlo
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Preparar el historial para la API de Gemini
    # El rol 'assistant' se corresponde con el rol 'model' en Gemini.
    gemini_history = [
        {"role": "model" if msg["role"] == "assistant" else "user", "parts": [msg["content"]]}
        for msg in st.session_state.messages[:-1]  # Excluir el 칰ltimo mensaje del usuario
    ]

    # Generar la respuesta del modelo en streaming
    with st.chat_message("assistant"):
        # Iniciar una sesi칩n de chat con el historial
        chat = model_gemini.start_chat(history=gemini_history)
        # Enviar el nuevo mensaje del usuario junto con la instrucci칩n del sistema (stronger_prompt)
        stream = chat.send_message(f"{stronger_prompt}\n\n{prompt}", stream=True)
        
        # st.write_stream espera un generador de strings, no de objetos complejos.
        # Este generador extrae el texto de cada trozo que env칤a la API de Gemini.
        # Se a침ade un `try-except` para ignorar los trozos vac칤os al final del stream que causan el ValueError.
        def stream_generator(stream):
            for chunk in stream:
                try:
                    yield chunk.text
                except ValueError:
                    pass # Ignora los trozos que no tienen texto.
        response = st.write_stream(stream_generator(stream))

    # A침adir la respuesta del asistente al historial
    st.session_state.messages.append({"role": "assistant", "content": response})